---
title: |
  | Assignment II: 
  | Adopting an Open Science Workflow \vspace{1cm}
author: |
  | Melisa Mazaeva 
  | Accounting Reading Group – Summer 2024
  | Humboldt-Universität zu Berlin 
  | melisa.mazaeva@student.hu-berlin.de
date: today
date-format: MMM D, YYYY [\vspace{1cm}]
abstract: |
  | This project uses the TRR 266 Template for Reproducible Empirical Accounting Research (TREAT) and provides an infrastructure for open science oriented empirical projects. It has been adapted to analyze residual income based on grouped P/B ratios of U.S. public firms using external Worldscope data sets. It showcases a reproducible workflow integrating Python scripts and data analysis. The analysis is performed using a Python script that processes input data files from Wolrdscope and generates output documentation files with the results. This code base, adapted from TREAT, should give you an overview on how the template is supposed to be used for my specific project and how to structure a reproducible empirical project.
  | \vspace{6cm}
bibliography: references.bib
biblio-style: apsr
format:
  pdf:
    documentclass: article
    number-sections: true
    toc: false
fig_caption: yes
fontsize: 11pt
ident: yes
always_allow_html: yes
header-includes:
  - \usepackage[nolists]{endfloat}    
  - \usepackage{setspace}\doublespacing
  - \setlength{\parindent}{4em}
  - \setlength{\parskip}{0em}
  - \usepackage[hang,flushmargin]{footmisc}
  - \usepackage{caption} 
  - \captionsetup[table]{skip=24pt,font=bf}
  - \usepackage{array}
  - \usepackage{threeparttable}
  - \usepackage{adjustbox}
  - \usepackage{graphicx}
  - \usepackage{csquotes}
  - \usepackage{indentfirst}  # Added this line to ensure the first paragraph is indented
  - \usepackage[margin=1in]{geometry}
---

\pagebreak

# List of Abbreviations
\setlength{\parindent}{0em}

**AMEX**: American Stock Exchange  
**CAPM**: Capital Asset Pricing Model  
**BVE**: Book Value of Equity  
**IDE**: Integrated Development Environment  
**MVE**: Market Value of Equity  
**NYSE**: New York Stock Exchange  
**P/B**: Price-to-Book  
**UK**: United Kingdom  
**US**: United States  

\setlength{\parindent}{4em} 

\pagebreak


# Introduction

The primary objective is to demonstrate a reproducible and collaborative research workflow by calculating and analyzing residual incomes and Price-to-Book (P/B) ratios. The analysis includes filtering relevant data, identifying key financial years, grouping firms based on their P/B ratios, and calculating delfated residual incomes for each firm/year.

Assignment I aims to replicate an empirical table that involves calculating the residual income of US firms over a defined period and examining the relationship between residual income and the Price-to-Book (P/B) ratio. The replication process includes data preparation, cleaning, normalization, and the application of statistical methods to compute and interpret financial metrics using Python. By following a step-by-step approach, I provide insights into the trends of residual earnings across different P/B groups, identical to trends presented by @Penman_2013. This document presents the analysis performed in Assignment I, adapted to the open science workflow. Note that intermediate check steps have been omitted for brevity. The full code, including intermediate print steps to verify outputs, is presented in `code/python/Assignment1_analyze_residual_income.py`.

# Assumptions and Notes

The aim of Assignment I is to replicate a specific empirical table that involves calculating the residual income of US firms over a defined period and examining the relationship between residual income and the Price-to-Book (P/B) ratio. The replication process involves data preparation, cleaning, and normalization, followed by the application of statistical methods to compute and interpret financial metrics. For Assignment I, I used the Python programming language to carry out the empirical analysis. Visual Studio Code was used as the Integrated Development Environment (IDE) for writing, debugging, and optimizing the Python code.

## Assumptions
1. A constant cost of equity is assumed to simplify the calculation of residual income across multiple years, making the analysis more straightforward and easier to interpret. This approach allows for a direct comparison of performance across firms and time periods without the increased complexity due to fluctuating rates. In contrary, estimating a dynamic cost of equity typically involves financial models such as the Capital Asset Pricing Model (CAPM), which would require additional data like risk-free rates and firm beta values that are not available.
2. The level of cost of equity capital is assumed to be 8% (re = 0.08), a reasonable average estimate for the period 1996-2015 in the US (@Goedhart_Koller_Williams_2002; @Damodaran_2024). This is assumption reflects the return equity investors typically expect given the level of risk during that period.
3. The entire Worldscope panel data provides the Book Value of Equity (BVE), Market Value of Equity (MVE), and net income as values for the end of the respective fiscal year.
4. The beginning of Year 0 data is equivalent to the end of the previous year because the financial data recorded at the end of one year represents the starting values for the following year. Therefore, residual income is deflated by the BVE at the end of the year before Year 0 to ensure comparability across different firms and time periods.
5. It is assumed that Penman (2013) restricted the possible P/B values to a range of 0 to 7 to exclude outliers. Hence, extreme P/B values that are negative and very high (greater than 7) are excluded to focus the analysis on firms with more stable and reasonable valuations, reducing the impact of outliers.

By following the steps provided in Section 4 and adhering to the assumptions made, I successfully replicated the analysis and produced the required table. A thorough step-by-step approach helped to understand and verify the outputs.

# Replication Steps
## Step 1: Data Loading and Merging
Begin by loading the data from the provided Worldscope files. The static data contains information on unique ISIN IDs for each firm, as well as the corresponding firm name and country. The panel data provides the financial performance numbers of each ISIN. Merge the data on the ‘isin’ column to include country information in the panel data. This merging step is crucial for filtering the US firms specifically in further steps.


```{python}
#| label: step1
#| echo: true
#| output: true

import pandas as pd
import numpy as np
import os
import yaml

# Function to get the path of the current file
def get_current_path():
    try:
        return os.path.dirname(os.path.abspath(__file__))
    except NameError:
        return os.getcwd()

# Define the relative path to the configuration file
current_path = get_current_path()
project_root = os.path.abspath(os.path.join(current_path, '..'))
config_path = os.path.join(project_root, 'config', 'config.yaml')

# Load the configuration from YAML file
with open(config_path, 'r') as file:
    config = yaml.safe_load(file)

# Extract paths from the config file
STATIC_DATA_PATH = os.path.join(project_root, 'data', 'external', 
                                'wscp_static.txt')
PANEL_DATA_PATH = os.path.join(project_root, 'data', 'external', 
                               'wscp_panel.xlsx')

# Load the data from Worldscope files into pandas DataFrames
static_data = pd.read_csv(STATIC_DATA_PATH, delimiter='\t')
panel_data = pd.read_excel(PANEL_DATA_PATH)

# Normalize column names to avoid issues with column naming conventions
static_data.columns = (
    static_data.columns.str.strip()
    .str.lower()
    .str.replace(' ', '_')
)

panel_data.columns = (
    panel_data.columns.str.strip()
    .str.lower()
    .str.replace(' ', '_')
)

# Merge data on 'isin' to include country information in panel_data
merged_data = panel_data.merge(
    static_data[['isin', 'country']], 
    on='isin', 
    how='left'
)
```

## Step 2: Filter Data for US Firms and Relevant Years
Filter the merged data to include only US firms (excluding UK and German firms) and the year range 1996-2015. This ensures that further calculations are based on the specific subset of data that matches the assignment’s criteria.
```{python}
#| label: step2
#| echo: true
#| output: true

us_firms = merged_data[merged_data['country'] == 'UNITED STATES']  
filtered_data = us_firms[
    (us_firms['year_'] >= 1996) & 
    (us_firms['year_'] <= 2015)
].copy() 
```

## Step 3: Identify Year 0 to Year 6 for Each Firm
To calculate the P/B values and residual incomes for each firm/year, it is necessary to identify the base year (Year 0) and the subsequent relative years within the range 0 – 6 in the filtered data, as presented by Penman (2013). Define Year 0 as the earliest year in the dataset for each firm and display it in a new column ‘year_0’ that holds this value, making ‘year_0’ the same for each ISIN. Additionally, create the column ‘relative_year’, represented as the difference between the current year and Year 0. Since a firm may have more than 7 years of observations in the filtered data, limit the data to include only the first seven years (Year 0 to Year 6) for each ISIN to match the assignment’s requirements.
Notably, the number of observations after merging Step 1 (298,120), filtering Step 2 (142,837), and identifying years in Step 3 (89,584) nearly halves with each step.

```{python}
#| label: step3
#| echo: true
#| output: true

filtered_data['year_0'] = filtered_data.groupby('isin')['year_'].transform('min')  
filtered_data['relative_year'] = filtered_data['year_'] - filtered_data['year_0']  
filtered_data = filtered_data[filtered_data['relative_year'] <= 6]  
```

## Step 4: Calculate P/B Ratio for Each Firm in Year 0
According to Penman (2013), the residual earnings are calculated after P/B groups are formed in Year 0. Therefore, consider the firms’ financial performance data only for Year 0 to calculate the P/B ratio. Replace zero and infinite BVE values with NaN to drop invalid calculations
(affecting 28 observations in total). Use the following formula to calculate the P/B ratio:
$$
P/B_t = \frac{\textit{Market value of equity}_t}{\textit{Book value of equity}_t}
$$

```{python}
#| label: step4
#| echo: true
#| output: true

year_0_data = filtered_data[filtered_data['relative_year'] == 0].copy() 

# replace zero and very small BVE values with NaN
year_0_data['bve'] = year_0_data['bve'].replace([0, np.inf, -np.inf], np.nan)  

# Use P/B Ratio formula to calculate it for each firm in Year 0
year_0_data['p/b'] = year_0_data['mve'] / year_0_data['bve']

# Remove rows with NaN P/B ratios
year_0_data = year_0_data.dropna(subset=['p/b'])
```

## Step 5: Form P/B Groups
Sort the firms based on their P/B ratios in descending order and divide them into 20 P/B groups, with the highest P/B ratios in group 1 and the lowest in group 20, as presented by Penman (2013). As a result, each P/B group contains approximately 900 firms, ensuring an equal division of the data set. Calculate the median P/B value for each group and merge the P/B group assignment information back into the filtered data, providing information on which P/B group each firm belongs to in each year.
```{python}
#| label: step5
#| echo: true
#| output: true

# Sort firms based on their P/B ratios for Year 0 in descending order
num_groups = 20  # define the number of groups
year_0_data = year_0_data.sort_values(
    'p/b', ascending=False
).reset_index(drop=True)

# Assign the groups so that the highest P/B ratios are in group 1
year_0_data['p/b_group'] = pd.qcut(
    year_0_data.index,
    num_groups,
    labels=range(1, num_groups + 1)
)

# Calculate median P/B value for each group
median_pb_values = year_0_data.groupby('p/b_group', observed=True)['p/b'].median()

# Merge the P/B groups back to the filtered data for Year 0
filtered_data = filtered_data.merge(
    year_0_data[['isin', 'p/b_group']],
    on='isin',
    how='left'
)

# Fill missing P/B groups for years other than Year 0 with same group as Year 0
filtered_data['p/b_group'] = filtered_data.groupby('isin')['p/b_group'].ffill()
```

## Step 6: Calculate Residual Income
This step involves calculating the residual income for each P/B group for Years 0 to 6, followed by deflation by the BVE at the beginning of Year 0. Assume a cost of equity capital of 8%, a reasonable assumption for US firms in the given time range. Based on Penman (2013), the residual income for each company/year should be deflated by the book value of equity at the beginning of Year 0.

To identify whether the financial data from Worldscope contains the numbers as of the beginning or end of the fiscal year, a random sample check was conducted. For example, for ISIN AN8068571086 in its *2020 Annual Report* (2021), the values for total assets, stockholders’ equity, net income, and other financial metrics are measured at the end of each fiscal year [@Schlumberger_2021, p. 20]. Hence, assume that the entire Worldscope panel data provides the BVE, MVE, and net income as values for the end of the respective fiscal year.

In case a company has observations before 1996, the filtered data set does not enable calculation of residual income in Year 0 (1996) due to missing value of BVE in Year -1, resulting into NaN. To avoid that, create a copy of the merged data from Step 1 to preserve all years of observations. Filter it to US companies and identify the year_minus_1 for each firm. Extract BVE for the year before Year 0 (relative_year = -1) for the deflation step and extract the BVE of the previous year for each year for the residual income calculation step.

Calculate the non-deflated residual income for each firm/year using the following formula:
$$
\text{Residual income}_t = \text{Net income}_t - r_e \times \text{Book value of equity}_{t-1}
$$

Assume that the beginning of Year 0 is equivalent to the end of the previous year because the financial data recorded at the end of one year represents the starting values for the following year. According to Penman (2013), residual income is deflated by book value at the beginning of year 0. Hence, for deflating the residual income, use the book value of equity from the year before Year 0 (bve_deflation) for all years (Years 1 to 6). This means that the BVE used for deflation remains constant across all years, normalizing the residual income across different firms and time periods. For example, for Year 0 (1996), use the BVE at the end of 1995 to deflate the residual income.

Calculate the deflated residual income for each firm/year using the following general formula:

$$
\text{Deflated residual income}_t = \frac{\text{Residual income}_t}{\text{Book value of equity}_{\text{Year 0 start}}}
$$

```{python}
#| label: step6
#| echo: true
#| output: true

cost_of_equity = 0.08  # assumption for cost of equity capital

# Create a copy of the merged data from Step 1 to keep all years of observations
us_full_data = merged_data[merged_data['country'] == 'UNITED STATES'].copy()

# Identify Year 0 for each firm, ensuring Year 0 is between 1996 and 2015
us_full_data['year_0'] = us_full_data.groupby('isin')['year_'].transform(
    lambda x: x[(x >= 1996) & (x <= 2015)].min()
)
us_full_data = us_full_data[us_full_data['year_0'].notna()] 
# Calculate the relative year and year_minus_1
us_full_data['relative_year'] = us_full_data['year_'] - us_full_data['year_0']
us_full_data['year_minus_1'] = us_full_data['year_0'] - 1

# Convert years to integer
us_full_data['year_0'] = us_full_data['year_0'].astype(int)
us_full_data['relative_year'] = us_full_data['relative_year'].astype(int)
us_full_data['year_minus_1'] = us_full_data['year_minus_1'].astype(int)

# Extract BVE for the year before Year 0 (relative_year = -1) for deflation
bve_deflation = (
    us_full_data[us_full_data['relative_year'] == -1][['isin', 'bve']]
    .rename(columns={'bve': 'bve_deflation'})
)

# Merge the bve_deflation values back to the filtered data
filtered_data = filtered_data.merge(bve_deflation, on='isin', how='left')

# Calculate the BVE of the previous year for each year
us_full_data['bve_prev'] = us_full_data.groupby('isin')['bve'].shift(1)
# Merge bve_prev back to filtered_data
filtered_data = filtered_data.merge(
    us_full_data[['isin', 'year_', 'bve_prev']],
    on=['isin', 'year_'],
    how='left'
)

# Function to calculate non-deflated residual income
def calculate_residual_income(row):
    return row['ninc'] - (cost_of_equity * row['bve_prev'])
# Apply the function to calculate non-deflated residual income for each row
filtered_data['residual_income'] = filtered_data.apply(
    calculate_residual_income, axis=1
)

# Function to deflate residual income
def deflate_residual_income(row):
    if pd.notna(row['bve_deflation']) and row['bve_deflation'] != 0:
        return row['residual_income'] / row['bve_deflation']
    else:
        return np.nan
# Apply the function to deflate residual income for each row
filtered_data['deflated_residual_income'] = filtered_data.apply(
    deflate_residual_income, axis=1
)
```

## Step 7: Output the Replicated Table
Calculate the median deflated residual income for each P/B group and relative year. Include the median P/B values in the output table. Ensure consistency with Penman (2013) by formatting the results to three decimal places for residual income and two decimal places for P/B values. The resulting table will be saved into an Excel file in the `data/generated` folder of the repository.

The output table certainly indicates a wide range of P/B ratios among the firms, with some extreme values in high and low P/B groups. There is a noticeable trend where firms with very high or very low P/B ratios have less stable and more extreme residual earnings over time, while middle P/B groups seem to show more stable trends as presented by Penman (2013), suggesting a balanced reflection of expectations between market valuation and expected financial performance.

The variation in trends can be explained by the difference in data sets and possible additional assumptions made by Penman (2013) that are not available. For example, the assignment data set includes all USA firms from the Worldscope database, providing a broader sample than Penman’s original study, which included only NYSE and AMEX firms. Furthermore, the assignment analysis focuses on the years 1996 to 2015, a more recent period with different economic conditions compared to the time frame used in Penman’s (2013) study.
```{python}
#| label: step7
#| echo: true
#| output: true

median_residual_income = filtered_data.groupby(
    ['p/b_group', 'relative_year'], observed=True
)['deflated_residual_income'].median().unstack()

# Formatting the results (decimal places)
median_residual_income = median_residual_income.apply(
    lambda col: col.map(
        lambda x: f"{x:.3f}" if pd.notna(x) else "NaN"
    )
)
median_pb_values = median_pb_values.apply(lambda x: f"{x:.2f}")

# Add the median P/B values column to the output table
median_residual_income.insert(0, 'P/B', median_pb_values)

# Display the median residual income table
print(
    "Median Residual Income by P/B Group and Relative Year:\n", 
    median_residual_income
)
```

## Step 8: Modification - Exclude Extreme P/B Values
As indicated in Step 7, the output table certainly includes outliers. To replicate the more stable P/B values observed by Penman (2013), exclude extreme P/B values that are negative or higher than 7. Recalculate the P/B groups based on this filtered data. Merge the updated P/B group information back into the filtered data and repeat the calculation of median deflated residual income for each P/B group and relative year. Format the updated results with the required decimal places and save to an Excel file. The modified output of the replicated table, closely resembling Penman (2013), is presented in Table 2.

The modified table resembles more the one of Penman (2013) by presenting similar trends. In particular, the exclusion of extreme P/B values results in more stable and consistent residual earnings across P/B groups and years. High P/B groups show higher residual earnings initially, which slowly decline over time, and low P/B groups have negative residual earnings that tend to fluctuate but still remain negative.
```{python}
#| label: step8
#| echo: true
#| output: true

# Filter out extreme P/B values that are negative and higher than 7
filtered_year_0_data = year_0_data[
    (year_0_data['p/b'] > 0) & (year_0_data['p/b'] <= 7)
]

# Recalculate the P/B groups based on the filtered data
filtered_year_0_data = filtered_year_0_data.sort_values(
    'p/b', ascending=False
).reset_index(drop=True)

filtered_year_0_data['p/b_group'] = pd.qcut(
    filtered_year_0_data.index, num_groups, labels=range(1, num_groups + 1)
)

# Merge the P/B groups back to the filtered data for Year 0
filtered_data = filtered_data.drop(columns=['p/b_group'])
filtered_data = filtered_data.merge(
    filtered_year_0_data[['isin', 'p/b_group']], 
    on='isin', 
    how='left'
)
filtered_data['p/b_group'] = filtered_data.groupby('isin')['p/b_group'].ffill()

# Calculate median P/B values again
median_pb_values = filtered_year_0_data.groupby(
    'p/b_group', 
    observed=True
)['p/b'].median()

# Calculate the median deflated residual income for each group and year
median_residual_income_filtered = filtered_data.groupby(
    ['p/b_group', 'relative_year'], 
    observed=True
)['deflated_residual_income'].median().unstack()

# Formatting the results (decimal places)
median_residual_income_filtered = median_residual_income_filtered.apply(
    lambda col: col.map(lambda x: f"{x:.3f}" if pd.notna(x) else "NaN")
)
median_pb_values = median_pb_values.apply(lambda x: f"{x:.2f}")

# Add the median P/B values column to the output table
median_residual_income_filtered.insert(0, 'P/B', median_pb_values)

# Display the median residual income table
print(
    "Median Residual Income by P/B Group and Relative Year:\n",
    median_residual_income_filtered
)
```

# Conclusion
This project demonstrates the effectiveness of using an open science and collaborative workflow for analyzing residual incomes and Price-to-Book ratios of U.S. public firms. By following a step-by-step approach and using the TRR 266 Template for Reproducible Empirical Accounting Research, I was able to replicate an empirical table from @Penman_2013 and provide insights into the trends of residual earnings across different P/B groups, similar to @Penman_2013.

The analysis revealed that high P/B groups initially exhibit higher residual earnings, which slowly decline over time, while low P/B groups consistently show negative residual earnings. Excluding extreme P/B values resulted in more stable and consistent residual earnings across groups and years.

In the future, this repository can be cloned or forked (if made public) to kickstart further projects on residual income analysis. Thanks for reading and enjoy!

\pagebreak

\setcounter{table}{0}
\renewcommand{\thetable}{\arabic{table}}


# References {-}
\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}
\setlength{\parskip}{8pt}
\noindent